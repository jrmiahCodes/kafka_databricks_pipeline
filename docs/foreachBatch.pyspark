## THIS CODE MAKES BRONZE WRITE IDEMPOTENT, AND CACHES THE INITIAL READ SO SPARK DOESN'T HAVE TO RECOMPUTE UDFS OR PARSE LOGIC.
from pyspark.sql import DataFrame

def foreach_batch_fn(batch_df: DataFrame, batch_id: int):

    # Exit early if batch is empty (Spark Connectâ€“safe)
    if batch_df.isEmpty():
        return
    
    batch_df = batch_df.persist()

    try:
        # ------------------------
        # Bronze: raw append
        # ------------------------
        bronze_rows = (
            batch_df
            .selectExpr(
                "CAST(value AS STRING) AS json_payload",
                "timestamp AS kafka_timestamp",
                "topic",
                "partition",
                "offset"
            )
            .withColumn("ingest_ts", current_timestamp())
            .withColumn("parse_status", lit(None).cast("string"))
            .withColumn("parse_error", lit(None).cast("string"))
        )

        bronze_rows.createOrReplaceTempView("tmp_bronze")

        spark.sql(f"""
        MERGE INTO {BRONZE_TABLE} AS tgt
        USING tmp_bronze AS src
        ON tgt.topic = src.topic
        AND tgt.partition = src.partition
        AND tgt.offset = src.offset
        WHEN NOT MATCHED THEN INSERT *
        """)

        # ------------------------
        # Parse payload
        # ------------------------
        payload_schema = StructType([
            StructField("user", StringType(), True),
            StructField("message", StringType(), True),
            StructField("sentiment", StructType([
                StructField("neg", DoubleType(), True),
                StructField("neu", DoubleType(), True),
                StructField("pos", DoubleType(), True),
                StructField("compound", DoubleType(), True)
            ]), True)
        ])

        parsed = (
            batch_df
            .select(
                from_json(col("value").cast("string"), payload_schema).alias("p"),
                col("timestamp").alias("kafka_timestamp"),
                col("topic"),
                col("partition"),
                col("offset")
            )
        )

        # ------------------------
        # Good rows
        # ------------------------
        good = (
            parsed
            .filter(col("p").isNotNull())
            .select(
                col("p.user").alias("user"),
                col("p.message").alias("message"),
                col("p.sentiment.neg").alias("sent_neg"),
                col("p.sentiment.neu").alias("sent_neu"),
                col("p.sentiment.pos").alias("sent_pos"),
                col("p.sentiment.compound").alias("sent_compound"),
                col("kafka_timestamp"),
                col("topic"),
                col("partition"),
                col("offset")
            )
        )

        good_enriched = (
            good
            .withColumn("message_length", length(trim(col("message"))).cast("int"))
            .withColumn("entropy", entropy_udf(col("message")))
            .withColumn("repeat_char_ratio", repeat_char_ratio_udf(col("message")))
            .withColumn("alpha_ratio", alpha_ratio_udf(col("message")))
            .withColumn("is_emote_only", is_emote_only_udf(col("message")) == lit("True"))
            .withColumn("emote_count", emote_count_udf(col("message")).cast("int"))
            .withColumn(
                "engagement_flag",
                (col("message_length") >= 2) &
                (col("entropy") >= 1.0) &
                (col("repeat_char_ratio") < 0.7)
            )
            .withColumn("ingest_ts", current_timestamp())
        )
        
        silver_rows = good_enriched

        silver_rows.createOrReplaceTempView("tmp_silver")

        spark.sql(f"""
        MERGE INTO {SILVER_TABLE} AS tgt
        USING tmp_silver AS src
        ON tgt.topic = src.topic
        AND tgt.partition = src.partition
        AND tgt.offset = src.offset
        WHEN NOT MATCHED THEN INSERT *
        """)

        # ------------------------
        # Update Bronze: OK rows
        # ------------------------
        good_keys = (
            good
            .select("topic", "partition", "offset")
            .dropDuplicates()
            .withColumn("parse_status", lit("OK"))
            .withColumn("parse_error", lit(None).cast("string"))
        )

        good_keys.createOrReplaceTempView("tmp_good_keys")

        spark.sql(f"""
        MERGE INTO {BRONZE_TABLE} AS tgt
        USING tmp_good_keys AS src
        ON tgt.topic = src.topic
        AND tgt.partition = src.partition
        AND tgt.offset = src.offset
        WHEN MATCHED THEN UPDATE SET
        tgt.parse_status = src.parse_status,
        tgt.parse_error = src.parse_error
        """)

        # ------------------------
        # Update Bronze: PARSE_ERROR rows
        # ------------------------
        bad = parsed.filter(col("p").isNull())

        if not bad.isEmpty():
            error_rows = (
                bad
                .select("topic", "partition", "offset")
                .dropDuplicates()
                .withColumn("parse_error", lit("PARSE_ERROR"))
                .withColumn("parse_status", lit("PARSE_ERROR"))
            )

            error_rows.createOrReplaceTempView("tmp_error_keys")

            spark.sql(f"""
            MERGE INTO {BRONZE_TABLE} AS tgt
            USING tmp_error_keys AS src
            ON tgt.topic = src.topic
            AND tgt.partition = src.partition
            AND tgt.offset = src.offset
            WHEN MATCHED THEN UPDATE SET
            tgt.parse_status = src.parse_status,
            tgt.parse_error = src.parse_error
            """)
    finally:
        batch_df.unpersist()