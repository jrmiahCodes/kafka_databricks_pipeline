# Databricks Notebook (PySpark) - Kafka -> Bronze (raw) -> Silver (parsed) with foreachBatch
# CELL 1: Imports & config
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, lit, current_timestamp, length, trim, regexp_replace,
    split, size, lower, udf, expr
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, TimestampType, LongType, IntegerType
)
import math
import re

spark = SparkSession.builder.getOrCreate()

# --------------------------
# CELL 2: Define table identifiers (Unity Catalog style)
BRONZE_TABLE = "main.twitch_pipeline_bronze.twitch_chat_raw"
SILVER_TABLE = "main.twitch_pipeline_silver.twitch_chat_parsed"
# checkpoint locations (DBFS)
BRONZE_CHECKPOINT = "/mnt/bronze/_checkpoints/twitch_chat"
SILVER_CHECKPOINT = "/mnt/silver/_checkpoints/twitch_chat_parsed"

# --------------------------
# CELL 3: Create Bronze and Silver tables if not exists (Delta)
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (
  json_payload STRING,
  kafka_timestamp TIMESTAMP,
  topic STRING,
  partition INT,
  offset LONG,
  ingest_ts TIMESTAMP,
  parse_status STRING,
  parse_error STRING
) USING DELTA
""")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {SILVER_TABLE} (
  user STRING,
  message STRING,
  sent_neg DOUBLE,
  sent_neu DOUBLE,
  sent_pos DOUBLE,
  sent_compound DOUBLE,
  message_length INT,
  entropy DOUBLE,
  is_emote_only BOOLEAN,
  emote_count INT,
  repeat_char_ratio DOUBLE,
  alpha_ratio DOUBLE,
  engagement_flag BOOLEAN,
  kafka_timestamp TIMESTAMP,
  topic STRING,
  partition INT,
  offset LONG,
  ingest_ts TIMESTAMP
) USING DELTA
""")

# --------------------------
# CELL 4: UDFs: entropy, repeat-char-ratio, alpha_ratio, is_emote_only, emote_count, simple spam heuristics

def _shannon_entropy(s):
    if s is None or len(s) == 0:
        return 0.0
    freq = {}
    for ch in s:
        freq[ch] = freq.get(ch, 0) + 1
    entropy = 0.0
    ln = float(len(s))
    for v in freq.values():
        p = v / ln
        entropy -= p * math.log2(p)
    return float(entropy)

def _repeat_char_ratio(s):
    if s is None or len(s) == 0:
        return 0.0
    max_run = 1
    run = 1
    prev = s[0]
    for ch in s[1:]:
        if ch == prev:
            run += 1
            if run > max_run:
                max_run = run
        else:
            run = 1
            prev = ch
    return float(max_run) / float(len(s))

def _alpha_ratio(s):
    if s is None or len(s) == 0:
        return 0.0
    alpha = sum(1 for ch in s if ch.isalpha())
    return float(alpha) / float(len(s))

# Sample emote list (expand as needed)
_EMOTE_LIST = {
    "Kappa":  -0.2,
    "PogChamp": 0.8,
    "OMEGALUL": 0.7,
    "LUL": 0.6,
    "Kreygasm": 0.7,
    "4Head": 0.1,
    "FeelsBadMan": -0.6,
    "FeelsGoodMan": 0.6
}
EMOTE_SET = set(_EMOTE_LIST.keys())
EMOTE_SENTIMENT = _EMOTE_LIST

def _emote_tokens(message):
    if message is None:
        return []
    # split on whitespace, strip punctuation
    tokens = [re.sub(r'^[^\w]+|[^\w]+$', '', t) for t in message.split()]
    tokens = [t for t in tokens if t != ""]
    return tokens

def _is_emote_only(message):
    tokens = _emote_tokens(message)
    if not tokens:
        return False
    for t in tokens:
        if t not in EMOTE_SET:
            return False
    return True

def _emote_count(message):
    tokens = _emote_tokens(message)
    return sum(1 for t in tokens if t in EMOTE_SET)

def _emote_sentiment_score(message):
    tokens = _emote_tokens(message)
    score = 0.0
    for t in tokens:
        if t in EMOTE_SENTIMENT:
            score += EMOTE_SENTIMENT[t]
    return float(score)

entropy_udf = udf(_shannon_entropy, DoubleType())
repeat_char_ratio_udf = udf(_repeat_char_ratio, DoubleType())
alpha_ratio_udf = udf(_alpha_ratio, DoubleType())
is_emote_only_udf = udf(_is_emote_only, StringType())  # will cast to boolean later
emote_count_udf = udf(_emote_count, IntegerType())
emote_sentiment_udf = udf(_emote_sentiment_score, DoubleType())

# --------------------------
# CELL 5: Kafka streaming read (raw) - note bootstrap servers + topic must be configured for your env
KAFKA_BOOTSTRAP = "your_kafka_bootstrap:9092"  # replace with actual
KAFKA_TOPIC = "twitch_chat_enriched"

raw_kafka_df = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

bronze_batch_schema_preview = raw_kafka_df.printSchema()

# --------------------------
# CELL 6: ForeachBatch function to append to Bronze, parse, write Silver, and update Bronze parse_status
from pyspark.sql import DataFrame

def foreach_batch_fn(batch_df: DataFrame, batch_id: int):
    if batch_df.rdd.isEmpty():
        return

    # Prepare bronze rows
    bronze_rows = batch_df.selectExpr(
        "CAST(value AS STRING) AS json_payload",
        "timestamp AS kafka_timestamp",
        "topic AS topic",
        "partition AS partition",
        "offset AS offset"
    ).withColumn("ingest_ts", current_timestamp()) \
     .withColumn("parse_status", lit(None).cast("string")) \
     .withColumn("parse_error", lit(None).cast("string"))

    # Append raw rows to bronze (append)
    bronze_rows.write.format("delta").mode("append").saveAsTable(BRONZE_TABLE)

    # Try parsing the payload using from_json
    payload_schema = StructType([
        StructField("user", StringType(), True),
        StructField("message", StringType(), True),
        StructField("sentiment", StructType([
            StructField("neg", DoubleType(), True),
            StructField("neu", DoubleType(), True),
            StructField("pos", DoubleType(), True),
            StructField("compound", DoubleType(), True)
        ]), True)
    ])

    parsed = batch_df.select(
        from_json(col("value").cast("string"), payload_schema).alias("p"),
        col("timestamp").alias("kafka_timestamp"),
        col("topic"),
        col("partition"),
        col("offset")
    )

    # Good (parsed) and bad (parse error = p is null)
    good = parsed.filter(col("p").isNotNull()).select(
        col("p.user").alias("user"),
        col("p.message").alias("message"),
        col("p.sentiment.neg").alias("sent_neg"),
        col("p.sentiment.neu").alias("sent_neu"),
        col("p.sentiment.pos").alias("sent_pos"),
        col("p.sentiment.compound").alias("sent_compound"),
        col("kafka_timestamp"),
        col("topic"),
        col("partition"),
        col("offset")
    )

    bad = parsed.filter(col("p").isNull()).select(
        col("kafka_timestamp"),
        col("topic"),
        col("partition"),
        col("offset")
    ).withColumn("parse_error", lit("PARSE_ERROR"))

    # Enrich good with computed columns
    good_enriched = good \
        .withColumn("message_length", length(trim(col("message"))).cast("int")) \
        .withColumn("entropy", entropy_udf(col("message"))) \
        .withColumn("repeat_char_ratio", repeat_char_ratio_udf(col("message"))) \
        .withColumn("alpha_ratio", alpha_ratio_udf(col("message"))) \
        .withColumn("is_emote_only", (is_emote_only_udf(col("message")) == lit("True"))) \
        .withColumn("emote_count", emote_count_udf(col("message")).cast("int")) \
        .withColumn("engagement_flag", (
            (col("message_length") >= 2) &
            (col("entropy") >= 1.0) &                       # example threshold
            (col("repeat_char_ratio") < 0.5)
        )) \
        .withColumn("ingest_ts", current_timestamp())

    # Append to Silver
    good_enriched.write.format("delta").mode("append").saveAsTable(SILVER_TABLE)

    # Update Bronze parse_status for good rows -> OK
    good_keys = good.select("topic", "partition", "offset").dropDuplicates()
    good_keys.createOrReplaceTempView("tmp_good_keys")

    spark.sql(f"""
    MERGE INTO {BRONZE_TABLE} AS tgt
    USING tmp_good_keys AS src
      ON tgt.topic = src.topic AND tgt.partition = src.partition AND tgt.offset = src.offset
    WHEN MATCHED THEN UPDATE SET tgt.parse_status = 'OK', tgt.parse_error = NULL
    """)

    # Update Bronze parse_status for bad rows -> PARSE_ERROR
    if bad.rdd.isEmpty():
        pass
    else:
        bad_keys = bad.select("topic", "partition", "offset").dropDuplicates()
        bad_keys.createOrReplaceTempView("tmp_bad_keys")
        spark.sql(f"""
        MERGE INTO {BRONZE_TABLE} AS tgt
        USING tmp_bad_keys AS src
          ON tgt.topic = src.topic AND tgt.partition = src.partition AND tgt.offset = src.offset
        WHEN MATCHED THEN UPDATE SET tgt.parse_status = 'PARSE_ERROR', tgt.parse_error = 'PARSE_ERROR'
        """)

# --------------------------
# CELL 7: Start the streaming query with foreachBatch to ensure Bronze append + Silver parsing + Bronze status updates
query = raw_kafka_df.writeStream \
    .foreachBatch(foreach_batch_fn) \
    .option("checkpointLocation", BRONZE_CHECKPOINT) \
    .start()

# To stop the stream later: query.stop()
